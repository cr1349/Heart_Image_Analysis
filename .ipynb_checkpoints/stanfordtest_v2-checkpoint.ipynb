{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import PIL.Image\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import pandas as pd\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, BatchNormalization, Activation\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.constraints import maxnorm\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import PIL.Image\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from keras.preprocessing import image\n",
    "from skimage.io import imread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(r'C:\\Users\\Christopher\\Documents\\Python_Scripts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['.\\\\Col_sub_images\\\\Injured\\\\inj_1_1.png',\n",
       "       '.\\\\Col_sub_images\\\\Injured\\\\inj_1_2.png',\n",
       "       '.\\\\Col_sub_images\\\\Injured\\\\inj_1_3.png',\n",
       "       '.\\\\Col_sub_images\\\\Injured\\\\inj_1_4.png',\n",
       "       '.\\\\Col_sub_images\\\\Injured\\\\inj_1_5.png',\n",
       "       '.\\\\Col_sub_images\\\\Injured\\\\inj_1_6.png',\n",
       "       '.\\\\Col_sub_images\\\\Injured\\\\inj_2_1.png',\n",
       "       '.\\\\Col_sub_images\\\\Injured\\\\inj_2_2.png',\n",
       "       '.\\\\Col_sub_images\\\\Injured\\\\inj_2_3.png',\n",
       "       '.\\\\Col_sub_images\\\\Injured\\\\inj_2_4.png',\n",
       "       '.\\\\Col_sub_images\\\\Injured\\\\inj_2_5.png',\n",
       "       '.\\\\Col_sub_images\\\\Injured\\\\inj_2_6.png',\n",
       "       '.\\\\Col_sub_images\\\\Injured\\\\inj_3_1.png',\n",
       "       '.\\\\Col_sub_images\\\\Injured\\\\inj_3_2.png',\n",
       "       '.\\\\Col_sub_images\\\\Injured\\\\inj_3_3.png',\n",
       "       '.\\\\Col_sub_images\\\\Injured\\\\inj_3_4.png',\n",
       "       '.\\\\Col_sub_images\\\\Injured\\\\inj_3_5.png',\n",
       "       '.\\\\Col_sub_images\\\\Injured\\\\inj_3_6.png',\n",
       "       '.\\\\Col_sub_images\\\\Injured\\\\inj_4_1.png',\n",
       "       '.\\\\Col_sub_images\\\\Injured\\\\inj_4_2.png',\n",
       "       '.\\\\Col_sub_images\\\\Injured\\\\inj_4_3.png',\n",
       "       '.\\\\Col_sub_images\\\\Injured\\\\inj_4_4.png',\n",
       "       '.\\\\Col_sub_images\\\\Injured\\\\inj_4_5.png',\n",
       "       '.\\\\Col_sub_images\\\\Injured\\\\inj_4_6.png',\n",
       "       '.\\\\Col_sub_images\\\\Injured\\\\inj_5_1.png',\n",
       "       '.\\\\Col_sub_images\\\\Injured\\\\inj_5_2.png',\n",
       "       '.\\\\Col_sub_images\\\\Injured\\\\inj_5_3.png',\n",
       "       '.\\\\Col_sub_images\\\\Injured\\\\inj_5_4.png',\n",
       "       '.\\\\Col_sub_images\\\\Injured\\\\inj_5_5.png',\n",
       "       '.\\\\Col_sub_images\\\\Injured\\\\inj_5_6.png',\n",
       "       '.\\\\Col_sub_images\\\\Injured\\\\inj_6_1.png',\n",
       "       '.\\\\Col_sub_images\\\\Injured\\\\inj_6_2.png',\n",
       "       '.\\\\Col_sub_images\\\\Injured\\\\inj_6_3.png',\n",
       "       '.\\\\Col_sub_images\\\\Injured\\\\inj_6_4.png',\n",
       "       '.\\\\Col_sub_images\\\\Injured\\\\inj_6_5.png',\n",
       "       '.\\\\Col_sub_images\\\\Injured\\\\inj_6_6.png',\n",
       "       '.\\\\Col_sub_images\\\\Sham\\\\shm_1_1.png',\n",
       "       '.\\\\Col_sub_images\\\\Sham\\\\shm_1_2.png',\n",
       "       '.\\\\Col_sub_images\\\\Sham\\\\shm_1_3.png',\n",
       "       '.\\\\Col_sub_images\\\\Sham\\\\shm_1_4.png',\n",
       "       '.\\\\Col_sub_images\\\\Sham\\\\shm_1_5.png',\n",
       "       '.\\\\Col_sub_images\\\\Sham\\\\shm_1_6.png',\n",
       "       '.\\\\Col_sub_images\\\\Sham\\\\shm_2_1.png',\n",
       "       '.\\\\Col_sub_images\\\\Sham\\\\shm_2_2.png',\n",
       "       '.\\\\Col_sub_images\\\\Sham\\\\shm_2_3.png',\n",
       "       '.\\\\Col_sub_images\\\\Sham\\\\shm_2_4.png',\n",
       "       '.\\\\Col_sub_images\\\\Sham\\\\shm_2_5.png',\n",
       "       '.\\\\Col_sub_images\\\\Sham\\\\shm_2_6.png',\n",
       "       '.\\\\Col_sub_images\\\\Sham\\\\shm_3_1.png',\n",
       "       '.\\\\Col_sub_images\\\\Sham\\\\shm_3_2.png',\n",
       "       '.\\\\Col_sub_images\\\\Sham\\\\shm_3_3.png',\n",
       "       '.\\\\Col_sub_images\\\\Sham\\\\shm_3_4.png',\n",
       "       '.\\\\Col_sub_images\\\\Sham\\\\shm_3_5.png',\n",
       "       '.\\\\Col_sub_images\\\\Sham\\\\shm_3_6.png',\n",
       "       '.\\\\Col_sub_images\\\\Sham\\\\shm_4_1.png',\n",
       "       '.\\\\Col_sub_images\\\\Sham\\\\shm_4_2.png',\n",
       "       '.\\\\Col_sub_images\\\\Sham\\\\shm_4_3.png',\n",
       "       '.\\\\Col_sub_images\\\\Sham\\\\shm_4_4.png',\n",
       "       '.\\\\Col_sub_images\\\\Sham\\\\shm_4_5.png',\n",
       "       '.\\\\Col_sub_images\\\\Sham\\\\shm_4_6.png',\n",
       "       '.\\\\Col_sub_images\\\\Sham\\\\shm_5_1.png',\n",
       "       '.\\\\Col_sub_images\\\\Sham\\\\shm_5_2.png',\n",
       "       '.\\\\Col_sub_images\\\\Sham\\\\shm_5_3.png',\n",
       "       '.\\\\Col_sub_images\\\\Sham\\\\shm_5_4.png',\n",
       "       '.\\\\Col_sub_images\\\\Sham\\\\shm_5_5.png',\n",
       "       '.\\\\Col_sub_images\\\\Sham\\\\shm_5_6.png',\n",
       "       '.\\\\Col_sub_images\\\\Sham\\\\shm_6_1.png',\n",
       "       '.\\\\Col_sub_images\\\\Sham\\\\shm_6_2.png',\n",
       "       '.\\\\Col_sub_images\\\\Sham\\\\shm_6_3.png',\n",
       "       '.\\\\Col_sub_images\\\\Sham\\\\shm_6_4.png',\n",
       "       '.\\\\Col_sub_images\\\\Sham\\\\shm_6_5.png',\n",
       "       '.\\\\Col_sub_images\\\\Sham\\\\shm_6_6.png'], dtype='<U36')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list = []\n",
    "\n",
    "Col_sub_images = \".\"\n",
    "\n",
    "def getListOfFiles(Col_sub_images):\n",
    "   \n",
    "    listOfFile = os.listdir(Col_sub_images)\n",
    "    allFiles = list()\n",
    "   \n",
    "    for entry in listOfFile:\n",
    "        # Create full path\n",
    "        fullPath = os.path.join(Col_sub_images, entry)\n",
    "       \n",
    "        if os.path.isdir(fullPath):\n",
    "            allFiles = allFiles + getListOfFiles(fullPath)\n",
    "        else:\n",
    "            allFiles.append(fullPath)\n",
    "                \n",
    "    return allFiles  \n",
    "\n",
    "def main():\n",
    "    \n",
    "    dirName = '.';\n",
    "    \n",
    "   \n",
    "    listOfFiles = getListOfFiles(dirName)\n",
    "  \n",
    "    \n",
    "    \n",
    "    listOfFiles = list()\n",
    "    for (dirpath, dirnames, filenames) in os.walk(dirName):\n",
    "        listOfFiles += [os.path.join(dirpath, file) for file in filenames]\n",
    "        \n",
    "        \n",
    "    # Append the files to the list    \n",
    "    for elem in listOfFiles:\n",
    "        file_list.append(elem) \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    \n",
    "file_list = np.asarray(file_list)\n",
    "file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_list = []\n",
    "x = 1\n",
    "y = 0                      # initially had labels as \"injured\" and \"sham\", however the NN would not take strings, so changed to 1 & 0 where 1= injured and 0= sham\n",
    "labels_list.extend([x]*36)\n",
    "labels_list.extend([y]*36)\n",
    "\n",
    "labels_list = np.asarray(labels_list)\n",
    "labels_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_function(filename, label):\n",
    "    image_string = tf.io.read_file(filename)\n",
    "    print(filename)\n",
    "    #Don't use tf.image.decode_image, or the output shape will be undefined\n",
    "    image = tf.image.decode_png(image_string, channels=3)\n",
    "\n",
    "    #This will convert to float values in [0, 1]\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    \n",
    "    image = tf.image.resize(image, [100 , 100])\n",
    "    \n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 6\n",
    "train_size = int(0.7 * len(file_list))\n",
    "val_size = int(0.15 * len(file_list))\n",
    "test_size = int(0.15 * len(file_list))\n",
    "\n",
    "full_dataset = tf.data.Dataset.from_tensor_slices((file_list, labels_list))\n",
    "full_dataset = full_dataset.cache()\n",
    "full_dataset = full_dataset.map(parse_function, num_parallel_calls=4) # invokes parse function to transform dataset to float32 and provide shape\n",
    "full_dataset = full_dataset.shuffle(len(file_list))\n",
    "#full_dataset = full_dataset.map(train_preprocess, num_parallel_calls=4) #applies train_preprocess which is not needed\n",
    "full_dataset = full_dataset.prefetch(1) #increases speed at cost of more RAM usage\n",
    "\n",
    "train_dataset = full_dataset.take(train_size) #to be used to fit the model & train\n",
    "test_dataset = full_dataset.skip(train_size)\n",
    "val_dataset = test_dataset.skip(val_size)\n",
    "test_dataset = test_dataset.take(test_size)\n",
    "test_dataset = test_dataset.prefetch(1)\n",
    "\n",
    "#train_dataset = full_dataset.map(parse_function, num_parallel_calls=4)\n",
    "train_dataset = full_dataset.batch(batch_size)\n",
    "train_dataset = train_dataset.prefetch(1)\n",
    "\n",
    "#val_dataset = full_dataset.map(parse_function, num_parallel_calls=4)\n",
    "val_dataset = full_dataset.batch(batch_size)\n",
    "val_dataset = val_dataset.prefetch(1)\n",
    "\n",
    "#test_dataset = full_dataset.map(parse_function, num_parallel_calls=4)\n",
    "test_dataset = full_dataset.batch(batch_size)\n",
    "test_dataset = test_dataset.prefetch(1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for element in test_dataset: \n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "\n",
    "max_pool = tf.keras.layers.MaxPool2D((2, 2), (2, 2), padding='same')\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(\n",
    "        filters=16,\n",
    "        kernel_size=5,\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu, \n",
    "    input_shape=(100, 100, 3)), #changed input shape to match that of my files - 100x100 pixels, RGB\n",
    "    max_pool,\n",
    "    tf.keras.layers.Conv2D(\n",
    "        filters=32,\n",
    "        kernel_size=5,\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu),\n",
    "    max_pool,\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 0.0230 - accuracy: 1.0000 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "12/12 [==============================] - 1s 59ms/step - loss: 0.0079 - accuracy: 1.0000 - val_loss: 0.0026 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "12/12 [==============================] - 1s 59ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.0032 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 7.3471e-04 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 0.0230 - accuracy: 0.9861 - val_loss: 0.0604 - val_accuracy: 0.9861\n",
      "Epoch 7/10\n",
      "12/12 [==============================] - 1s 59ms/step - loss: 0.0517 - accuracy: 1.0000 - val_loss: 0.0131 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 0.0362 - accuracy: 0.9861 - val_loss: 0.0037 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.0199 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x18bf8f7aac0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    epochs = 10,\n",
    "    validation_data = val_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = model.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 at  0.0%. model identifies image as category 0 with a confidence of (100.0%)\n",
      "2 at  0.0%. model identifies image as category 1 with a confidence of (100.0%)\n",
      "3 at  0.0%. model identifies image as category 0 with a confidence of (100.0%)\n",
      "4 at  0.0%. model identifies image as category 1 with a confidence of (99.9%)\n",
      "5 at  0.0%. model identifies image as category 0 with a confidence of (99.5%)\n",
      "6 at  0.0%. model identifies image as category 0 with a confidence of (100.0%)\n",
      "7 at  0.0%. model identifies image as category 1 with a confidence of (99.4%)\n",
      "8 at  0.0%. model identifies image as category 0 with a confidence of (100.0%)\n",
      "9 at  0.0%. model identifies image as category 1 with a confidence of (98.8%)\n",
      "injured. model identifies image as category 1 with a confidence of (99.9%)\n",
      "Where 1 = Injured, 0 = Uninjured\n"
     ]
    }
   ],
   "source": [
    "truths = list(range(1, 10)) + [\"injured\"]\n",
    "\n",
    "table = []\n",
    "for truth, probs in zip(truths, probabilities):\n",
    "    prediction = probs.argmax()\n",
    "    if truth == 'injured':\n",
    "        print(f\"{truth}. model identifies image as category {prediction} with a confidence of ({probs[prediction]*100:.1f}%)\")\n",
    "    else:\n",
    "        print(f\"{truth} at {probs[truth]*100:4.1f}%. model identifies image as category {prediction} with a confidence of ({probs[prediction]*100:4.1f}%)\")\n",
    "    table.append((truth, probs))\n",
    "    \n",
    "print(\"Where 1 = Injured, 0 = Uninjured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
